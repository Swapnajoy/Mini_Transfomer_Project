# ðŸ“Œ Overview
This project explores the architecture, training, and inference of large language models from scratch using PyTorch. It also revisits the LSTM architecture from the pre-transformer era to provide historical and performance-based comparison.

The following model types are implemented and analyzed:

Encoder-only Transformers (e.g., BERT-style)

Decoder-only Transformers (e.g., GPT-style)

Full Seq2Seq Transformers (e.g., for machine translation)

Baseline RNN model using LSTM

Document the observations during:

  training for loss curves, overfitting, plateauing etc. Experiment with the hyperparameters.
  
  inference, how the generated text compares with the other models. Generalization capability.
  
Visualize the results

Finally build a mini transformer based language model.

In Progress
