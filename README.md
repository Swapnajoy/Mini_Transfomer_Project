# ðŸ“Œ Overview
This project explores the architecture, training and inference of large language models from scratch using PyTorch. It also focusses on the LSTM architecture from the pre-transfomre era for comparison. The following have been covered:

Encoder-only models (e.g., BERT-style)

Decoder-only models (e.g., GPT-style)

Full Seq2Seq models (Transformer-based translation)

Baseline RNN model using LSTMs

Document the observations during:

  training for loss curves, overfitting, plateauing etc. Experiment with the hyperparameters.
  
  inference, how the generated text compares with the other models. Generalization capability.
  
Visualize the results

Finally build a mini transformer based language model.

In Progress
