{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192873ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config_lstm' from 'c:\\\\wriju\\\\Aachen\\\\Academic\\\\Git\\\\Mini_Transfomer_Project\\\\config_lstm.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from utils.ch_tokenizer import CharTokenizer\n",
    "\n",
    "from models.encoder_only.enc_only_transformer import EncoderOnlyTransformer\n",
    "from models.decoder_only.dec_only_transformer import DecoderOnlyTransformer\n",
    "\n",
    "import config_encoder_only as cfg_enc\n",
    "import config_decoder_only as cfg_dec\n",
    "import config_lstm as cfg_lstm\n",
    "\n",
    "import importlib\n",
    "importlib.reload(cfg_enc)\n",
    "importlib.reload(cfg_dec)\n",
    "importlib.reload(cfg_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72abbb2",
   "metadata": {},
   "source": [
    "Function to plot attention weights in a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee76482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_layer(attn_weights, tokens):\n",
    "\n",
    "    num_layers = len(attn_weights)\n",
    "    num_heads = attn_weights[0].size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(num_layers, num_heads, figsize=(3*num_heads, 3*num_layers))\n",
    "    fig.suptitle(\"Attention Maps per Layer and Head\", fontsize=16)\n",
    "    \n",
    "    for layer_idx, attn_tensor in enumerate(attn_weights):\n",
    "        attn_tensor = attn_tensor.squeeze(0).cpu()\n",
    "        \n",
    "        for head_idx in range(num_heads):\n",
    "            ax = axs[layer_idx, head_idx] if num_layers > 1 else axs[head_idx]\n",
    "            sns.heatmap(attn_tensor[head_idx], xticklabels=tokens, yticklabels=tokens,\n",
    "                        cmap=\"viridis\", ax=ax, cbar=False)\n",
    "            ax.set_title(f\"L{layer_idx+1} H{head_idx+1}\")\n",
    "            ax.tick_params(axis='x', labelrotation=90)\n",
    "            ax.set_xlabel(\"Key\")\n",
    "            ax.set_ylabel(\"Query\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e8fe5",
   "metadata": {},
   "source": [
    "Functions to plot training loss, validation loss and perplexity over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02f7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(training_info_path):\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    with open(training_info_path, 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            content = line.strip().split(',')\n",
    "            epochs.append(int(content[0]))\n",
    "            train_loss.append(round(float(content[1]), 3))\n",
    "            val_loss.append(round(float(content[2]), 3))\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.title('Training Loss and Validation Loss')\n",
    "    plt.plot(epochs, train_loss, marker='o', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, marker='x', label='Validation_Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0743199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(training_info_path):\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    perplexity = []\n",
    "\n",
    "    with open(training_info_path, 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            content = line.strip().split(',')\n",
    "            epochs.append(int(content[0]))\n",
    "            train_loss.append(round(float(content[1]), 3))\n",
    "            val_loss.append(round(float(content[2]), 3))\n",
    "            perplexity.append(round(float(content[3]), 3))\n",
    "\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.suptitle('Training Loss, Validation Loss and Perplexity')\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(epochs, train_loss, marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(epochs, val_loss, marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(epochs, perplexity, marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7487727",
   "metadata": {},
   "source": [
    "LSTM: Visualize Training Loss, Validation Loss and Perplexity over the Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6803594",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_experiments/lstm/ep20_b64_lr5e-05_dataset_alice_in_wonderland.txt_token_ch/training_info.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m training_info_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_experiments/lstm/ep20_b64_lr5e-05_dataset_alice_in_wonderland.txt_token_ch/training_info.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplot_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_info_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mplot_losses\u001b[1;34m(training_info_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_info_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mnext\u001b[39m(file)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n",
      "File \u001b[1;32mc:\\Users\\sahas\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_experiments/lstm/ep20_b64_lr5e-05_dataset_alice_in_wonderland.txt_token_ch/training_info.txt'"
     ]
    }
   ],
   "source": [
    "training_info_path = \"training_experiments/lstm/ep20_b64_lr5e-05_dataset_alice_in_wonderland.txt_token_ch/training_info.txt\"\n",
    "plot_losses(training_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c5e73",
   "metadata": {},
   "source": [
    "Encoder-only Transformer: Visualize Training Loss, Validation Loss and Perplexity over the Epochs, and Attention-Weights as heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5311b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint_path = cfg_enc.CHECKPOINT_PATH\n",
    "txt_file_path = cfg_enc.TXT_FILE_PATH\n",
    "\n",
    "tokenizer = CharTokenizer(txt_file_path)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "seq_len = cfg_enc.SEQ_LEN\n",
    "embed_dim = cfg_enc.MODEL_CONFIG['embed_dim']\n",
    "num_heads = cfg_enc.MODEL_CONFIG['num_heads']\n",
    "hidden_dim = cfg_enc.MODEL_CONFIG['hidden_dim']\n",
    "enc_ffn_h_dim = cfg_enc.MODEL_CONFIG['enc_ffn_h_dim']\n",
    "num_enc = cfg_enc.MODEL_CONFIG['num_enc']\n",
    "use_sinusoidal = cfg_enc.MODEL_CONFIG['use_sinusoidal']\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EncoderOnlyTransformer(vocab_size, embed_dim, seq_len, hidden_dim, num_heads, enc_ffn_h_dim, num_enc, use_sinusoidal).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111afc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info_path = os.path.relpath(os.path.join(os.path.dirname(checkpoint_path), 'training_info.txt'))\n",
    "\n",
    "plot_losses(training_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "seed_text = \"Alice and her sister\"\n",
    "input_tokens = tokenizer.encode(seed_text)\n",
    "\n",
    "if len(input_tokens) > seq_len:\n",
    "    input_tokens = torch.tensor(input_tokens[-seq_len:]).unsqueeze(0).to(device)\n",
    "else:    \n",
    "    input_tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        pred_logits, attention_weights = model(input_tokens)\n",
    "\n",
    "        tokens = tokenizer.decode(input_tokens[0].tolist())\n",
    "\n",
    "        plot_attention_layer(attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f2490",
   "metadata": {},
   "source": [
    "Decoder-only Transformer: Visualize Training Loss, Validation Loss and Perplexity over the Epochs, and Attention-Weights as heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40853e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = cfg_dec.CHECKPOINT_PATH\n",
    "txt_file_path = cfg_dec.TXT_FILE_PATH\n",
    "\n",
    "tokenizer = CharTokenizer(txt_file_path)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "seq_len = cfg_dec.SEQ_LEN\n",
    "embed_dim = cfg_dec.MODEL_CONFIG['embed_dim']\n",
    "num_heads = cfg_dec.MODEL_CONFIG['num_heads']\n",
    "hidden_dim = cfg_dec.MODEL_CONFIG['hidden_dim']\n",
    "dec_ffn_h_dim = cfg_dec.MODEL_CONFIG['dec_ffn_h_dim']\n",
    "num_dec = cfg_dec.MODEL_CONFIG['num_dec']\n",
    "use_sinusoidal = cfg_dec.MODEL_CONFIG['use_sinusoidal']\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DecoderOnlyTransformer(vocab_size, embed_dim, seq_len, hidden_dim, num_heads, dec_ffn_h_dim, num_dec, use_sinusoidal).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info_path = os.path.relpath(os.path.join(os.path.dirname(checkpoint_path), 'training_info.txt'))\n",
    "\n",
    "plot_losses(training_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "seed_text = \"He who shan't be named\"\n",
    "input_tokens = tokenizer.encode(seed_text)\n",
    "\n",
    "if len(input_tokens) > seq_len:\n",
    "    input_tokens = torch.tensor(input_tokens[-seq_len:]).unsqueeze(0).to(device)\n",
    "else:    \n",
    "    input_tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        pred_logits, attention_weights = model(input_tokens)\n",
    "\n",
    "        tokens = tokenizer.decode(input_tokens[0].tolist())\n",
    "\n",
    "        plot_attention_layer(attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
